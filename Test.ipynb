{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Model-based RL don't need a value function for the policy\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=yellow>#대표적인 어간 추출(stemming) 기법인 Porter 및 Lancaster 추출 패키지를 불러오고 활용해봄 / Lemmatizer는 표제어 추출</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer       : ['eat', 'ate', 'eaten', 'eat']\n",
      "Lancaster Stemmer    : ['eat', 'at', 'eat', 'eat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "stem1 = PorterStemmer()\n",
    "stem2 = LancasterStemmer()\n",
    "words = [\"eat\",\"ate\",\"eaten\",\"eating\"]\n",
    "print(\"Porter Stemmer       :\", [stem1.stem(w) for w  in words])\n",
    "print(\"Lancaster Stemmer    :\", [stem2.stem(v) for v  in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatizer: ['eat', 'eat', 'eat', 'eat']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "words = [\"eat\",\"ate\",\"eaten\",\"eating\"]\n",
    "print(\"WordNet Lemmatizer:\",[lemm.lemmatize(v,pos=\"v\") for v in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=yellow>#불용어 제거</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'should', 'all', 'study', 'hard', 'for', 'the', 'exam', '.']\n",
      "['We', 'study', 'hard', 'exam', '.']\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"We should all study hard for the exam.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(input_sentence)\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=yellow>#정수 인코딩 및 Sorting</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course : English, Number : 0\n",
      "Course : Math, Number : 1\n",
      "Course : Science, Number : 2\n"
     ]
    }
   ],
   "source": [
    "mylist = ['English', 'Math', 'Science']\n",
    "for n, name in enumerate(mylist):\n",
    "    print(\"Course : {}, Number : {}\".format(name, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cup', 8), ('July', 6), ('piano', 4), ('apple', 2), ('orange', 1)]\n",
      "{'cup': 1, 'July': 2, 'piano': 3, 'apple': 4, 'orange': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab = {'apple': 2, 'July': 6, 'piano': 4, 'cup': 8, 'orange': 1}\n",
    "vocab_sort = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sort)\n",
    "word2inx = {word[0] : index + 1 for index, word in enumerate(vocab_sort)}\n",
    "print(word2inx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model-based': 0, 'RL': 1, 'do': 2, \"n't\": 3, 'need': 4, 'a': 5, 'value': 6, 'function': 7, 'for': 8, 'the': 9, 'policybut': 10, 'some': 11, 'of': 12, 'algorithms': 13, 'have': 14}\n",
      "[2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Model-based RL don't need a value function for the policy\" \\\n",
    "       \"but some of of Model-based RL algorithms do have a value function\"\n",
    "token_text = tokenizer.tokenize(text)\n",
    "word2inx = {}\n",
    "Bow = []\n",
    "for word in token_text:\n",
    "    if word not in word2inx.keys():\n",
    "        word2inx[word] = len(word2inx)\n",
    "        Bow.insert(len(word2inx)-1,1)\n",
    "    else:\n",
    "        inx = word2inx.get(word)\n",
    "        Bow[inx] += 1\n",
    "print(word2inx)\n",
    "print(Bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=yellow>#유사도 분석</font>     1. 코사인 유사도  2.레반슈타인 거리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067811865475 0.7071067811865475 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cos_sim(A, B):\n",
    "    return np.dot(A, B) / (np.linalg.norm(A)*np.linalg.norm(B))\n",
    "\n",
    "a = [1,0,0,1]\n",
    "b = [1,1,1,1]\n",
    "c = [0,1,1,0]\n",
    "print(cos_sim(a,b), cos_sim(b,c), cos_sim(c,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "def leven(text1, text2):\n",
    "    len1 = len(text1) + 1\n",
    "    len2 = len(text2) + 1\n",
    "    sim_array = np.zeros((len1,len2))\n",
    "    sim_array[:,0] = np.linspace(0, len1-1, len1)\n",
    "    sim_array[0,:] = np.linspace(0, len2-1, len2)\n",
    "    for i in range (1,len1):\n",
    "        for j in range (1,len2):\n",
    "            add_char = sim_array[i-1, j] + 1\n",
    "            sub_char = sim_array[i, j-1] + 1\n",
    "            if text1[i-1] == text2[j-1]:\n",
    "                mod_char = sim_array[i-1, j-1]\n",
    "            else:\n",
    "                mod_char = sim_array[i-1, j-1] + 1\n",
    "            sim_array[i, j] = min([add_char, sub_char, mod_char])\n",
    "    return sim_array[-1, -1]\n",
    "\n",
    "print(leven('datatmining', 'dtatamining'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=yellow>#Word2Vec</font> CBoW & SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:  transcript    0\n",
      "url           0\n",
      "dtype: int64\n",
      "Total word count:  1511916\n",
      "Good morning. How are you?(Laughter)It's\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "data = pd.read_csv('transcripts.csv')\n",
    "print('Missing Values: ', data.isnull().sum())\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "merge_data = ' '.join(str(data.iloc[i,0]) for i in range(100))\n",
    "print('Total word count: ', len(merge_data))\n",
    "print(merge_data[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
